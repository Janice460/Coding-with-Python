{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFbPvfyu3yaS"
   },
   "source": [
    "# Web Scraping con Python\n",
    "Janice Esocbedo Vasquez, 21/01/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "## 1. Conceptos\n",
    "### 1.1 Que es web scraping\n",
    "La practica de recolpialr datos mediante un programa automatixado que consulta u servidor web\\\n",
    "El web crowling o indexacion se utiliza para indexar la informacion de la pagina mediante bots\\\n",
    "Un bot rastrea un sittion web recorre todos las pagina s y todos los enlaces hasta la ultima linea del sitio web\n",
    "### 1.2 API (Interfaz de programacion de aplicaciones)\n",
    "Cualquier progrmaa o pagina wb úeden tener un servicio software con el cual nosotros ppodemos inteeractuar  y eso es un api.\\\n",
    "| Web scraping | API |\n",
    "| --- | --- |\n",
    "| Estrae informacion de un sitio web usando un programa informatico | Provee cceso a los datos de una aplicacion sistena operativo u otro sevicio |\\\n",
    "Mismo objetivo: accede a ls datos del sitio web\n",
    "Ai queremos informaciondebemos pregutnarm¿nos: Tiene una api?\\\n",
    "- SI: ¿Esa api me da la info que necesito?\n",
    "    - SI: Puedo haeer todos los pedidos que necesito?\n",
    "        - SI: Usamos la api\n",
    "        - NO: Scrapear\n",
    "    - NO: Scrapear\n",
    "- NO: Scrapear \n",
    "\n",
    "### 1.3 Conceptos basicos sobre la web\n",
    "#### HTTPS\n",
    "Protocolo seguro de transferencia de hipertexto que permite a los navegadores comunicarse con los servidores web (donde se almacenan los sitos)\\\n",
    "         Atraves de  get,ost, ETC\\\n",
    "Cliente ------ > Pedido (rewuest) Servidor\\\n",
    "  <-------- Respuesta (responsed)\\\n",
    "\n",
    "#### HTTP Status Codes\n",
    "(min 10)\n",
    "#### Mediante que formatos se comparte informacion de internet\n",
    "- CSV\n",
    "- JSON: Javascript Object Notations (apis)\n",
    "- XML: etiquetas, como html\n",
    "\n",
    "#### Tencnologias de una pagina web\n",
    "- HTML: Estructura, lenguaje de marcaddo de hipertexto\n",
    "- JS: Funcionalidad\n",
    "- CSS: Estilo, hojas de estilo en cascada\n",
    "\n",
    "#### DOM (Document object model)\n",
    "interfaz independiente del lenguaje que trata un documento XML y HTML como una estructura de arbol\n",
    "\n",
    "#### JavaScript\n",
    "Permite usar un paradigma de programacion orientada a eventos (entre otros). \\\n",
    "Mas del 97\\% de las paginas web lo usan para generar comportamientos interactivos\n",
    "\n",
    "### 1.4 Web Scrapiing con python\n",
    "Python no es la única tecnología con la cual podemos hacer scraping \n",
    "- R: rvest\n",
    "- JavaScript: Puppeteer\n",
    "- Java: Jsoud\n",
    "\n",
    "Python nos puede ayuuydar a muchas cosas\n",
    "- Pedidos HTTP (urllib, request)\n",
    "- Parseo de la informacion (BeautifulSoup)\n",
    "- Autoamatización\n",
    "- Control de excepciones\n",
    "\n",
    "#### Parsear la infromacion\n",
    "Dividir un txto en us componentes y describir sus roles sintacticos\n",
    "El 'parseo' de un documento HTML es baasciamente tomar codigo HTML y extraer información relevante como el titulo de la pagina, parrafo, encabezado, enlaces texto en negrita, etc\n",
    "\n",
    "#### Flujo de trabajo\n",
    "Pedir informacion de la pagina del servidor (HTTP request) ---> Parseamos el HTML (u otro formato que recibamos) ---> Procesamos la infromacion y la guardamos\n",
    "\n",
    "#### Parseando con python Beautiful soup\n",
    "- BeautifulDoup es una libreria para web scraping\n",
    "- Se usa para extraer datos de archivos HTML y XML\n",
    "- Creat un arbol de analisis a partir del codigo fuente de la pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T6lp5_w9Gi0"
   },
   "source": [
    "## 2. Preámbulo\n",
    "### 2.1 Pedidos HTTP con requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "32ZIp-taK6PE"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc1hU64tLv-v"
   },
   "outputs": [],
   "source": [
    "# Hacemos un pedido a la página de wikipedia\n",
    "URL = 'https://es.wikipedia.org/'\n",
    "\n",
    "# Guardamos el objeto que nos devuelve\n",
    "respuesta = requests.get(URL)\n",
    "\n",
    "# print(f'Tipo de Objeto: {type(respuesta)} \\n')\n",
    "# print(f'Código de estado: {respuesta.status_code} \\n')\n",
    "print(f'Data: {respuesta.text} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1AgS5HxPIPg"
   },
   "source": [
    "#### Headers\n",
    "\n",
    "Una serie de datos que acompañan al pedido. Para saber más: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers\n",
    "\n",
    "\n",
    "El objeto `Response` de `requests` tiene los siguientes elementos principales:\n",
    "\n",
    "* `.text`\n",
    "* `.content`\n",
    "* `.json()`\n",
    "* `.status_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HBifgfShPKlV"
   },
   "outputs": [],
   "source": [
    "URL = 'https://scrapepark.org/courses/spanish/'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "}\n",
    "respuesta = requests.get(URL, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbA4w5LXhb-F"
   },
   "outputs": [],
   "source": [
    "respuesta.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yF0A4Nl6K2d"
   },
   "source": [
    "Veamoslo en la práctica utilizando la siguiente web: http://httpbin.org/headers (útil para testear pedidos HTTP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGvs4upI6KLv"
   },
   "outputs": [],
   "source": [
    "URL = 'http://httpbin.org/headers'\n",
    "resp = requests.get(URL)\n",
    "\n",
    "print('Respuesta sin headers:')\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X94bg5Lfh1Uw"
   },
   "outputs": [],
   "source": [
    "print('Respuesta con headers:')\n",
    "nuestros_headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'\n",
    "    }\n",
    "resp_con_headers = requests.get(URL, headers = nuestros_headers)\n",
    "print(resp_con_headers.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVDnesOKZ7N"
   },
   "source": [
    "### 2.2 Uso basico de APIs\n",
    "#### Uso de API de manera directa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3rq7-cOLHuV"
   },
   "source": [
    "[Sunset and sunrise times API](https://sunrise-sunset.org/api)\n",
    "\n",
    "**Sirve para obtener la hora del amanecer y el ocaso de un determinado día**\n",
    "\n",
    "*Parámetros:*\n",
    "\n",
    "\n",
    "*  **lat** (float): Latitud en grados decimales(Obligatorio)\n",
    "*  **lng** (float): Longitud en grados decimales (obligatorio)\n",
    "*  **date** (string): Fecha en formato AAAA-MM-DD (opcional, por defecto usa el día actual)\n",
    "\n",
    "*Estructura de la query:* url base + parametros\n",
    "\n",
    "`https://api.sunrise-sunset.org/json?`\n",
    "\n",
    "`lat=36.7201600`\n",
    "\n",
    "`&`\n",
    "\n",
    "`lng=-4.4203400`\n",
    "\n",
    "`&`\n",
    "\n",
    "`date=2021-07-26`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EIFWdHPVJsEi"
   },
   "outputs": [],
   "source": [
    "# Definimos los parametros de nuestra query\n",
    "latitud = '36.7201600' # -34.6\n",
    "longitud = '-4.4203400' # -58.4\n",
    "fecha = '2021-07-26' # AAAA-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oe0bMEQBJubx"
   },
   "outputs": [],
   "source": [
    "# Hacemos el pedido y guardamos la respuesta en una nueva variable\n",
    "respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')\n",
    "# https://api.sunrise-sunset.org/json?lat=36.7201600&lng=-4.4203400&date=2021-07-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_sunset = {\"results\":\n",
    "                    {\"sunrise\":\"5:17:35 AM\",\"sunset\":\"7:30:52 PM\",\"solar_noon\":\"12:24:14 PM\",\"day_length\":\"14:13:17\",\"civil_twilight_begin\":\"4:50:01 AM\",\"civil_twilight_end\":\"7:58:26 PM\",\"nautical_twilight_begin\":\"4:14:31 AM\",\"nautical_twilight_end\":\"8:33:56 PM\",\"astronomical_twilight_begin\":\"3:35:59 AM\",\"astronomical_twilight_end\":\"9:12:28 PM\"},\"status\":\"OK\",\"tzid\":\"UTC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1KBTJWoYllvx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(respuesta_sunset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dZkY8OHKJxFN"
   },
   "outputs": [],
   "source": [
    "# Para des-serializar el objeto (que era tipo 'HTTPResponse') y cargarlo como json\n",
    "# datos_sunset = respuesta_sunset.json()\n",
    "# print(datos_sunset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7kr6nPiQl1gK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['results', 'status', 'tzid'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(datos_sunset)\n",
    "datos_sunset = respuesta_sunset\n",
    "datos_sunset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sunrise': '5:17:35 AM',\n",
       " 'sunset': '7:30:52 PM',\n",
       " 'solar_noon': '12:24:14 PM',\n",
       " 'day_length': '14:13:17',\n",
       " 'civil_twilight_begin': '4:50:01 AM',\n",
       " 'civil_twilight_end': '7:58:26 PM',\n",
       " 'nautical_twilight_begin': '4:14:31 AM',\n",
       " 'nautical_twilight_end': '8:33:56 PM',\n",
       " 'astronomical_twilight_begin': '3:35:59 AM',\n",
       " 'astronomical_twilight_end': '9:12:28 PM'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_sunset['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AVf-wnSKmAKg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7:30:52 PM'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_sunset['results']['sunset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Z8lcUaepJ2IJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: OK\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el status del pedido\n",
    "sunset_status = datos_sunset['status']\n",
    "print(f'Status: {sunset_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vDCpwey_J6J1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 2021-07-26 el sol se ocultó a las 7:30:52 PM (UTC)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver su contenido ya que es son diccionarios anidados:\n",
    "sunset = datos_sunset['results']['sunset']\n",
    "print(f'El {fecha} el sol se ocultó a las {sunset} (UTC)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_1N7IJIiKIIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterando data_sunset['results']:\n",
      "sunrise\n",
      "sunset\n",
      "solar_noon\n",
      "day_length\n",
      "civil_twilight_begin\n",
      "civil_twilight_end\n",
      "nautical_twilight_begin\n",
      "nautical_twilight_end\n",
      "astronomical_twilight_begin\n",
      "astronomical_twilight_end\n"
     ]
    }
   ],
   "source": [
    "# tambien podriamos iterar sobre sus claves\n",
    "print(\"Iterando data_sunset['results']:\")\n",
    "for elemento in datos_sunset['results']:\n",
    "    print(elemento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcHglp06hnfy"
   },
   "source": [
    "#### **Uso de API por medio de una librería: Wikipedia**\n",
    "\n",
    "Wikipedia-API es un *wrapper* de Python fácil de usar para la API de Wikipedia. Admite la extracción de textos, secciones, enlaces, categorías, traducciones, etc.\n",
    "\n",
    "Repositorio: https://github.com/martin-majlis/Wikipedia-API\n",
    "\n",
    "Documentación: https://wikipedia-api.readthedocs.io/en/latest/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EqWcjExWiihS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.2.2 from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip (python 3.9)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\JESCOBEDOV\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n",
      "botocore 1.27.28 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.1.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wikipedia-api==0.5.8\n",
      "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     -------------------------------------- 62.6/62.6 kB 670.9 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "     ------------------------------------ 100.4/100.4 kB 969.2 kB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.6/61.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "     -------------------------------------- 104.6/104.6 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "     ------------------------------------ 162.5/162.5 kB 698.6 kB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, wikipedia-api\n",
      "  Creating C:\\Users\\JESCOBEDOV\\AppData\\Roaming\\Python\\Python39\\Scripts\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.1.0 wikipedia-api-0.5.8\n"
     ]
    }
   ],
   "source": [
    "# Instalamos el paquete porque no viene con Colab\n",
    "!pip3 install --force-reinstall -v  \"wikipedia-api==0.5.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9WTgRAA3sjdG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "# Ahora si podemos importarlo\n",
    "import wikipediaapi\n",
    "\n",
    "# Chequear versión\n",
    "print(wikipediaapi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLY7a8imigip",
    "outputId": "6e7e4d43-9c13-4eec-fb46-3c2db4f6af2a"
   },
   "outputs": [],
   "source": [
    "# Instanciamos la clase wikipediaapi y utilizamos el metodo Wikipedia con el parametro de idioma\n",
    "IDIOMA = 'es'\n",
    "wiki_wiki = wikipediaapi.Wikipedia(IDIOMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_programacion es un objeto de tipo: \n",
      " \n",
      "<class 'wikipediaapi.WikipediaPage'>\n"
     ]
    }
   ],
   "source": [
    "# Usamos el metodo page para y hacemos un pedido con una palabra clave\n",
    "PALABRA_CLAVE = 'programación'\n",
    "wikipedia_programacion = wiki_wiki.page(PALABRA_CLAVE)\n",
    "\n",
    "print(f'wikipedia_programacion es un objeto de tipo: \\n \\n{type(wikipedia_programacion)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dK5pdTsYivt8"
   },
   "outputs": [],
   "source": [
    "# Resumen\n",
    "# print(wikipedia_programacion.title)\n",
    "# print(' ')\n",
    "# print(wikipedia_programacion.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "UhukUmpni1Au"
   },
   "outputs": [],
   "source": [
    "# Url completa\n",
    "# print(wikipedia_programacion.fullurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ZRRKjv8b4e"
   },
   "source": [
    "## 3. BeautifulSoup\n",
    "Documentación oficial: https://beautiful-soup-4.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGXRo8BWtIDc"
   },
   "source": [
    "### 3.1 Generalidades\n",
    "\n",
    "Vamos a practicar con https://scrapepark.org/spanish/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DemLUNc7d_W7"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "T5XplMATeVXQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de BeautifulSoup: 4.11.1\n",
      "Versión de requests: 2.28.1\n"
     ]
    }
   ],
   "source": [
    "# Versiones\n",
    "import bs4 # Solo para el chequeo\n",
    "print(\"Versión de BeautifulSoup:\",bs4.__version__)\n",
    "print(\"Versión de requests:\", requests.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OOJxCTeb2Sys"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting beautifulsoup4==4.11.2\n",
      "  Downloading beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\n",
      "     -------------------------------------- 129.4/129.4 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4==4.11.2) (2.3.1)\n",
      "Installing collected packages: beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.11.2\n"
     ]
    }
   ],
   "source": [
    "# En caso de no tener la versión que se usa en este curso\n",
    "!pip3 install beautifulsoup4==4.11.2\n",
    "# !pip3 install requests==2.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.verify = True\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "YLnDRuoftSDX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Empezamos el scraping\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE = 'https://scrapepark.org/courses/spanish/'\n",
    "pedido_obtenido = requests.get(URL_BASE, headers=headers, verify=False)\n",
    "html_obtenido = pedido_obtenido.content\n",
    "\n",
    "# 2. \"Parsear\" ese HTML\n",
    "soup = BeautifulSoup(html_obtenido, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "O1lXQ39Ao64s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS5-q5L-eLZ_"
   },
   "source": [
    "### 3.2 El método `find()`\n",
    "\n",
    "Nos permite quedarnos con la información asociada a una etiqueta de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Jo7AkWAhh23W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>¿Por qué comprar con nosotros?</h2>\n"
     ]
    }
   ],
   "source": [
    "primer_h2 = soup.find('h2')\n",
    "print(primer_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jP47spPBiYfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n",
      "¿Por qué comprar con nosotros?\n"
     ]
    }
   ],
   "source": [
    "# Solo el texto\n",
    "print(primer_h2.text)\n",
    "\n",
    "# equivalente a:\n",
    "print(soup.h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFO_d99OeavG"
   },
   "source": [
    "### 3.3 El método `find_all()`\n",
    "\n",
    "Busca **TODOS** los elementos de la página con esa etiqueta y devuelve una \"lista\" que los contiene (en realidad devuelve un objeto de la clase *bs4.element.ResultSet*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6YlYosUFmc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>, <h2>\r\n",
      "                  #Novedades\r\n",
      "                </h2>, <h2>\r\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>, <h2>\r\n",
      "            Testimonios de clientes\r\n",
      "          </h2>, <h2 class=\"heading-container\">\r\n",
      "          Tabla de precios\r\n",
      "        </h2>]\n"
     ]
    }
   ],
   "source": [
    "h2_todos = soup.find_all('h2')\n",
    "print(h2_todos) # devuelve lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Az5VWD8qh5XP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>]\n"
     ]
    }
   ],
   "source": [
    "# ARGUMENTOS\n",
    "# Si usamos el parametro limit = 1, emulamos al metodo find\n",
    "h2_uno_solo = soup.find_all('h2',limit=1)\n",
    "print(h2_uno_solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "g_pjutuZh9wz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n",
      "\r\n",
      "                  #Novedades\r\n",
      "                \n",
      "\r\n",
      "            Nuestros productos\n",
      "\n",
      "\r\n",
      "            Testimonios de clientes\r\n",
      "          \n",
      "\r\n",
      "          Tabla de precios\r\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Podemos iterar sobre el objeto\n",
    "for seccion in h2_todos:\n",
    "    print(seccion.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Vm5TjiHV4lCR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué comprar con nosotros?\n",
      "#Novedades\n",
      "Nuestrosproductos\n",
      "Testimonios de clientes\n",
      "Tabla de precios\n"
     ]
    }
   ],
   "source": [
    "# get_text() para más funcionalidades\n",
    "for seccion in h2_todos:\n",
    "    # limpiar los espacios en blanco\n",
    "    print(seccion.get_text(strip=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H936ZYAerBXJ"
   },
   "source": [
    "### 3.4 Utilizando atributos de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "GyPbqaejp4is"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"heading-container heading-center\" id=\"acerca\">\n",
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\" id=\"productos\">\n",
      "<h2>\r\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h3>Suscríbete para obtener descuentos y ofertas</h3>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h2>\r\n",
      "            Testimonios de clientes\r\n",
      "          </h2>\n",
      "</div>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Clase\n",
    "divs = soup.find_all('div', class_ = \"heading-container heading-center\")\n",
    "\n",
    "for div in divs:\n",
    "    print(div)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "fVxip8F0iEYA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Logo de ScrapePark.org\" src=\"images/logo.svg\" width=\"250\"/>,\n",
       " <img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>,\n",
       " <img alt=\"Variedad de patinetas en la tienda\" src=\"images/arrival-bg-store.png\"/>,\n",
       " <img alt=\"Patineta 1\" src=\"images/p1.png\"/>,\n",
       " <img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>,\n",
       " <img alt=\"Patineta 3\" src=\"images/p3.png\"/>,\n",
       " <img alt=\"Patineta 4\" src=\"images/p4.png\"/>,\n",
       " <img alt=\"Patineta 5\" src=\"images/p5.png\"/>,\n",
       " <img alt=\"Patineta 6\" src=\"images/p6.png\"/>,\n",
       " <img alt=\"Patineta 7\" src=\"images/p7.png\"/>,\n",
       " <img alt=\"Patineta 8\" src=\"images/p8.png\"/>,\n",
       " <img alt=\"Patineta 9\" src=\"images/p9.png\"/>,\n",
       " <img alt=\"Patineta 10\" src=\"images/p10.png\"/>,\n",
       " <img alt=\"Patineta 11\" src=\"images/p11.png\"/>,\n",
       " <img alt=\"Patineta 12\" src=\"images/p12.png\"/>,\n",
       " <img alt=\"Cliente 1\" src=\"images/client-one.png\"/>,\n",
       " <img alt=\"Cliente 2\" src=\"images/client-two.png\"/>,\n",
       " <img alt=\"Cliente 3\" src=\"images/client-three.png\"/>,\n",
       " <iframe src=\"table.html\" title=\"table_iframe\"></iframe>,\n",
       " <img alt=\"#\" src=\"images/logo.svg\" width=\"210\"/>,\n",
       " <img alt=\"Logo de freeCodeCamp\" class=\"freecodecamp-logo\" src=\"./images/freecodecamp-logo.png\"/>,\n",
       " <script src=\"js/jquery-3.4.1.min.js\"></script>,\n",
       " <script src=\"js/popper.min.js\"></script>,\n",
       " <script src=\"js/bootstrap.js\"></script>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todas las etiquetas que tengan el atributo \"src\"\n",
    "src_todos = soup.find_all(src=True)\n",
    "src_todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>\n",
      "<img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>\n"
     ]
    }
   ],
   "source": [
    "for elemento in src_todos:\n",
    "    if elemento['src'].endswith(\".jpg\"):\n",
    "        print(elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x000002CB6CF085C0>\n"
     ]
    }
   ],
   "source": [
    "print(enumerate(src_todos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "pm4eVZCW6jjD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/arrival-bg-store.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p7.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p8.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p9.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p11.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/p12.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/client-one.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/client-two.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/client-three.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/freecodecamp-logo.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#@title Ejercicio: Bajar todas las imagenes!\n",
    "\n",
    "url_imagenes = []\n",
    "\n",
    "for i, imagen in enumerate(src_todos):\n",
    "\n",
    "    if imagen['src'].endswith('png'):\n",
    "\n",
    "        print(imagen['src'])\n",
    "        r = requests.get(f\"https://scrapepark.org/courses/spanish/{imagen['src']}\", verify=False)\n",
    "\n",
    "        with open(f'imagen_{i}.png', 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxurw0jxiwm0"
   },
   "source": [
    "### 3.5 Tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "YyqtIpXMw9sx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<iframe src=\"table.html\" title=\"table_iframe\"></iframe>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iframe: una pagina dentro de mi pagina, en este caso lleva a la pagina table.html\n",
    "soup.find_all('iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table.html'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('iframe')[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qvrCha4Qwz1P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"es\">\n",
       "<head>\n",
       "<!-- Basic -->\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<!-- Mobile Metas -->\n",
       "<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n",
       "<!-- Site Metas -->\n",
       "<meta content=\"\" name=\"keywords\"/>\n",
       "<meta content=\"\" name=\"description\"/>\n",
       "<meta content=\"\" name=\"author\"/>\n",
       "<link href=\"images/favicon.png\" rel=\"shortcut icon\" type=\"\"/>\n",
       "<title>ScrapePark.org</title>\n",
       "<!-- bootstrap core css -->\n",
       "<link href=\"css/bootstrap.css\" rel=\"stylesheet\" type=\"text/css\">\n",
       "<!-- font awesome style -->\n",
       "<link href=\"css/font-awesome.min.css\" rel=\"stylesheet\"/>\n",
       "<!-- Custom styles for this template -->\n",
       "<link href=\"css/style.css\" rel=\"stylesheet\"/>\n",
       "<!-- responsive style -->\n",
       "<link href=\"css/responsive.css\" rel=\"stylesheet\"/>\n",
       "</link></head>\n",
       "<body style=\"overflow-x: scroll\">\n",
       "<table class=\"table table-bordered table-striped table-hover\">\n",
       "<thead class=\"thead-dark\">\n",
       "<tr>\n",
       "<th> </th>\n",
       "<th class=\"text-center\">Skate</th>\n",
       "<th class=\"text-center\">Cruiser</th>\n",
       "<th class=\"text-center\" style=\"color: red;\">Longboard</th>\n",
       "<th class=\"text-center\">Freeboard</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Azul</td>\n",
       "<td class=\"text-center\">$64</td>\n",
       "<td class=\"text-center\">$70</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$80</td>\n",
       "<td class=\"text-center\">$85</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Verde</td>\n",
       "<td class=\"text-center\">$69</td>\n",
       "<td class=\"text-center\">$75</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$85</td>\n",
       "<td class=\"text-center\">$90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Negro</td>\n",
       "<td class=\"text-center\">$74</td>\n",
       "<td class=\"text-center\">$80</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$90</td>\n",
       "<td class=\"text-center\">$95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Morado</td>\n",
       "<td class=\"text-center\">$55</td>\n",
       "<td class=\"text-center\">$60</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$62</td>\n",
       "<td class=\"text-center\">$72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Especial</td>\n",
       "<td class=\"text-center\">$110</td>\n",
       "<td class=\"text-center\">$125</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$150</td>\n",
       "<td class=\"text-center\">$167</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Información de tablas\n",
    "\n",
    "URL_BASE = 'https://scrapepark.org/courses/spanish'\n",
    "URL_TABLA = soup.find_all('iframe')[0]['src']\n",
    "\n",
    "request_tabla = requests.get(f'{URL_BASE}/{URL_TABLA}', verify=False)\n",
    "\n",
    "html_tabla = request_tabla.text\n",
    "soup_tabla = BeautifulSoup(html_tabla, \"html.parser\")\n",
    "soup_tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"table table-bordered table-striped table-hover\">\n",
       "<thead class=\"thead-dark\">\n",
       "<tr>\n",
       "<th> </th>\n",
       "<th class=\"text-center\">Skate</th>\n",
       "<th class=\"text-center\">Cruiser</th>\n",
       "<th class=\"text-center\" style=\"color: red;\">Longboard</th>\n",
       "<th class=\"text-center\">Freeboard</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Azul</td>\n",
       "<td class=\"text-center\">$64</td>\n",
       "<td class=\"text-center\">$70</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$80</td>\n",
       "<td class=\"text-center\">$85</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Verde</td>\n",
       "<td class=\"text-center\">$69</td>\n",
       "<td class=\"text-center\">$75</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$85</td>\n",
       "<td class=\"text-center\">$90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Negro</td>\n",
       "<td class=\"text-center\">$74</td>\n",
       "<td class=\"text-center\">$80</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$90</td>\n",
       "<td class=\"text-center\">$95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Morado</td>\n",
       "<td class=\"text-center\">$55</td>\n",
       "<td class=\"text-center\">$60</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$62</td>\n",
       "<td class=\"text-center\">$72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Especial</td>\n",
       "<td class=\"text-center\">$110</td>\n",
       "<td class=\"text-center\">$125</td>\n",
       "<td class=\"text-center\" style=\"color: red;\">$150</td>\n",
       "<td class=\"text-center\">$167</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_tabla.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Longboard', '$80', '$85', '$90', '$62', '$150']\n"
     ]
    }
   ],
   "source": [
    "# th y td: filas y columnas\n",
    "# dentro de attrs: recibe un diccionario\n",
    "productos_faltantes = soup_tabla.find_all(['th', 'td'], attrs={'style':'color: red;'})\n",
    "productos_faltantes = [talle.text for talle in productos_faltantes]\n",
    "\n",
    "print(productos_faltantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "OWDrRpN14mkR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto: Patineta Nueva 1 | precio: 75\n",
      "Producto: Patineta Usada 2 | precio: 80\n",
      "Producto: Patineta Nueva 3 | precio: 68\n",
      "Producto: Patineta Usada 4 | precio: 70\n",
      "Producto: Patineta Nueva 5 | precio: 75\n",
      "Producto: Patineta Nueva 6 | precio: 58\n",
      "Producto: Patineta Nueva 7 | precio: 80\n",
      "Producto: Patineta Nueva 8 | precio: 35\n",
      "Producto: Patineta Nueva 9 | precio: 165\n",
      "Producto: Patineta Usada 10 | precio: 54\n",
      "Producto: Patineta Usada 11 | precio: 99\n",
      "Producto: Patineta Nueva 12 | precio: 110\n"
     ]
    }
   ],
   "source": [
    "divs = soup.find_all('div', class_='detail-box')\n",
    "productos = []\n",
    "precios = []\n",
    "\n",
    "for div in divs:\n",
    "    if (div.h6 is not None) and ('Patineta' in div.h5.text):\n",
    "        producto = div.h5.get_text(strip=True)\n",
    "        precio = div.h6.get_text(strip=True).replace('$', '')\n",
    "        # Se puede agregar filtros\n",
    "        print(f'Producto: {producto:<16} | precio: {precio}')\n",
    "        productos.append(producto)\n",
    "        precios.append(precio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFOxFOsRretL",
    "outputId": "5021ae88-400c-4cf0-e231-3a58c107eee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['75', '80', '68', '70', '75', '58', '80', '35', '165', '54', '99', '110']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "guSnGHLIrcp_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Patineta Nueva 1',\n",
       " 'Patineta Usada 2',\n",
       " 'Patineta Nueva 3',\n",
       " 'Patineta Usada 4',\n",
       " 'Patineta Nueva 5',\n",
       " 'Patineta Nueva 6',\n",
       " 'Patineta Nueva 7',\n",
       " 'Patineta Nueva 8',\n",
       " 'Patineta Nueva 9',\n",
       " 'Patineta Usada 10',\n",
       " 'Patineta Usada 11',\n",
       " 'Patineta Nueva 12']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QphGTciOnZS"
   },
   "source": [
    "### 3.6 Cambios que dependen de la URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "zW_XZj6TOqar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scrapepark.org/courses/spanish/contact1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto que cambia entre páginas en contacto 1 :)\n",
      "https://scrapepark.org/courses/spanish/contact2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto que cambia entre páginas en contacto 2 :)\n"
     ]
    }
   ],
   "source": [
    "URL_BASE = \"https://scrapepark.org/courses/spanish/contact\"\n",
    "\n",
    "for i in range(1,3):\n",
    "    URL_FINAL = f\"{URL_BASE}{i}\"\n",
    "    print(URL_FINAL)\n",
    "    r = requests.get(URL_FINAL, verify = False)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    print(soup.h5.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRw2htzlj9Wr"
   },
   "source": [
    "### 3.7 Datos que no sabemos en que parte de la página se encuentran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "SClwgT23kDb_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scrapepark.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' 4-444-4444']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expresiones regulares\n",
    "import re\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE = 'https://scrapepark.org/courses/spanish'\n",
    "pedido_obtenido = requests.get(URL_BASE, verify = False)\n",
    "html_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2. \"Parsear\" ese HTML\n",
    "soup = BeautifulSoup(html_obtenido, \"html.parser\")\n",
    "\n",
    "# \\d+ : digitos\n",
    "telefonos = soup.find_all(string=re.compile(\"\\d+-\\d+-\\d+\"))\n",
    "telefonos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sq_ZX03oPQ8"
   },
   "source": [
    "### 3.8 Moviéndonos por el árbol\n",
    "\n",
    "Para saber más: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "QqzYlp0jnxAA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'© 2022 '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyrights = soup.find_all(string=re.compile(\"©\"))\n",
    "copyrights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "BYaiCi_tuSek"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>© 2022 <span>Todos los derechos reservados</span>.\n",
       "        <a href=\"https://html.design/\" rel=\"noopener noreferrer\" target=\"_blank\">Creado con Free Html Templates</a>.\n",
       "      </p>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primer_copyright = copyrights[0]\n",
    "primer_copyright.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "GDlAB53SuDf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul>\n",
       " <li><a href=\"#\">Inicio</a></li>\n",
       " <li><a href=\"#\">Acerca</a></li>\n",
       " <li><a href=\"#\">Servicios</a></li>\n",
       " <li><a href=\"#\">Testimonios</a></li>\n",
       " <li><a href=\"#\">Contacto</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Otro ejemplo con elementos al mismo nivel\n",
    "menu = soup.find(string=re.compile(\"MENÚ\"))\n",
    "# menu.parent\n",
    "menu.parent.find_next_siblings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrUz6btarWj6"
   },
   "source": [
    "### 3.9 Comentario sobre excepciones\n",
    "https://docs.python.org/es/3/tutorial/errors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "iSgBLoDFrs3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENÚ\n",
      "© 2022 \n",
      "El string 'carpincho' no fue encontrado\n",
      "\r\n",
      "                  Patineta Nueva 1\r\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "strings_a_buscar = [\"MENÚ\", \"©\", \"carpincho\", \"Patineta\"]\n",
    "\n",
    "for string in strings_a_buscar:\n",
    "    try:\n",
    "        resultado = soup.find(string=re.compile(string))\n",
    "        print(resultado.text)\n",
    "    except AttributeError:\n",
    "        print(f\"El string '{string}' no fue encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnyPln5D4dsg"
   },
   "source": [
    "### 3.10 Almacenamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MizBsJA3gCQt"
   },
   "outputs": [],
   "source": [
    "productos.insert(0, \"productos\")\n",
    "precios.insert(0, \"precios\")\n",
    "# datos = dict(zip(productos, precios)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('productos', 'precios'),\n",
       " ('Patineta Nueva 1', '75'),\n",
       " ('Patineta Usada 2', '80'),\n",
       " ('Patineta Nueva 3', '68'),\n",
       " ('Patineta Usada 4', '70'),\n",
       " ('Patineta Nueva 5', '75'),\n",
       " ('Patineta Nueva 6', '58'),\n",
       " ('Patineta Nueva 7', '80'),\n",
       " ('Patineta Nueva 8', '35'),\n",
       " ('Patineta Nueva 9', '165'),\n",
       " ('Patineta Usada 10', '54'),\n",
       " ('Patineta Usada 11', '99'),\n",
       " ('Patineta Nueva 12', '110')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(productos, precios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "qXUvdB2Mx5WD"
   },
   "outputs": [],
   "source": [
    "datos = dict(zip(productos, precios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "tsss7E0Ny4mU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('productos', 'precios'), ('Patineta Nueva 1', '75'), ('Patineta Usada 2', '80'), ('Patineta Nueva 3', '68'), ('Patineta Usada 4', '70'), ('Patineta Nueva 5', '75'), ('Patineta Nueva 6', '58'), ('Patineta Nueva 7', '80'), ('Patineta Nueva 8', '35'), ('Patineta Nueva 9', '165'), ('Patineta Usada 10', '54'), ('Patineta Usada 11', '99'), ('Patineta Nueva 12', '110')])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "YcIts8u56tA7"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('datos.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(datos.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEHvwn9Xw0sf"
   },
   "source": [
    "**BONUS!**\n",
    "Algunos ejercicios para seguir practicando:\n",
    "\n",
    "1. Las patinetas que salgan menos que $68\n",
    "2. Las patinetas que en su nombre tengan un numero mayor a 3\n",
    "3. Traer cualquier texto de la pagina que tenga la palabra descuento u oferta.\n",
    "5. Generar un archivo .csv con dos columnas: Una conteniendo el nombre del cliente y otra su testimonio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "eCpy9QPW8Dwd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
